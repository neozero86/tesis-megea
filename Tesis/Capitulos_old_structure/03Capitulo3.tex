%---------------------------------------------------------------------
%
%                          Capítulo 3
%
%---------------------------------------------------------------------
%
\chapter{Métodos empleados}
\label{capMetodos}

\begin{resumen}
En este capítulo se describirán los métodos de \textit{machine learning} utilizados para el desarrollo de este trabajo.

\end{resumen}


\section{Introducción}
\label{capMetodos:sec:introduccion}

Dado que el principal objetivo de este trabajo es determinar si hay información almacenada en los \textit{spikes}, se utilizarán métodos estadísticos de clasificación para intentar clasificar los datos obtenidos en los ensayos.\\
Si se demuestra que con los datos recopilados de la actividad neuronal, se puede predecir (con cierta confianza) la respuesta del animal, entonces quedaría demostrado que existe información relevante almacenada en las neuronas.\\\\
Con esta meta se entrenaron todos los algoritmos estadísticos utilizados. 
\pagebreak
\section{Métodos}
\label{capMetodos:sec:métodos}

\subsection{Análisis Discriminante}

El primer método utilizado fue el análisis discriminante, el cual es una generalización del metodo descripto por \citep{fisher1936use}. Este método consiste en una técnica simple que permite encontrar una función dependiente de las variables para poder separar las clases que componen el conjunto. Asimismo, el mismo asume que las clases tendrán distribuciones Gaussianas separables entre sí, y si eso sucede se demuestra que el discriminante lineal de Fischer (FLD) es óptimo.
\\
\figura{Bitmap/03/LDA}{width=1\textwidth}{capMetodos:fig:LDA}%
{Separación lineal de 2 clases.}
El método descripto por \citep{fisher1936use} separa solamente dos clases. El LDA que se utiliza actualmente es una modificación del mismo que permite la separación de multiples clases. Este Multiclass LDA se enuncia en \citep{rao1948utilization}.\\
En este caso particular, se aplicó la version pseudolineal ya que la técnica del análisis discriminante requiere demasiado volumen de información para ajustar los modelos gaussianos con matrices de covarianza invertibles. En cambio, el modelo pseudolineal utiliza matrices de covarianza pseudoinversas, consiguiendo ajustar el modelo con menos volúmen de información. \citep{liu2007efficient}
\pagebreak
\subsection{Random Forest}
 
El método de Random Forest utiliza la técnica divide y conquista (\textit{divide and conquer}). Se trata de un ensamble de árboles de decisión entrenados con valores provenientes de un vector de muestras aleatorias, relevadas de forma independiente y con la misma distribución para todos los árboles.\citep[p. 1]{breiman2001random},\\
De esta manera, mediante un sistema de votos, se logra asignar la clase correspondiente a cada una de las muestras.\\\\
Los arboles de decision son predictores sin sesgo, pero tienen alta \mbox{varianza}. Random Forest busca reducir la varianza promediando un alto número de arboles de decisión \citep[p. 587]{hastie2009elements}. \\El error de generalización convergirá a un límite cuanto mayor sea el número de árboles. \citep[p. 1]{breiman2001random}
\\\\
Este método es un caso particular de \textit{bagging}, cuya idea principal es promediar muchos modelos con ruido y muy poco sesgo para conseguir reducir la varianza. Los árboles de decisión son candidatos ideales para \textit{bagging},  debido a que pueden capturar estructuras con interacción compleja en los datos y tienen relativamente bajo sesgo cuando crecen lo suficientemente profundo. Adicionalmente los árboles se benefician en gran medida con el promedio, dado que son muy sensibles al ruido en los datos. \citep[p. 282 - 288, p. 588]{hastie2009elements}
\\

Con el objetivo de una mejor comprensión del metodo se muestra a continuación la figura~\ref{capMetodos:fig:RF}.

\figura{Bitmap/03/RF}{width=1\textwidth}{capMetodos:fig:RF}%
{Proceso de clasificación de una muestra con Random Forest}

Cada variable (\textit{feature}) ``f'' de la muestra a clasificar, es evaluada en cada arbol, recorriendo en forma binaria las ramas, hasta llegar a una hoja. Estas hojas determinan la probabilidad que la muestra pertenezca a la clase ``c'', dado que esta posee el \textit{feature} ``f''. Esta probabilidad constituye el voto de cada arbol para la clase c. Recorriendo el vector de \textit{features} y sometiendo cada feature de la muestra al mismo proceso de votación, se obtiene, mediante la suma de todos los votos, la decisión de RF. \citep{kim2013fire}\\
 

La descripción completa de este método se encuentra en \citep{breiman2001random}. 
\pagebreak   
\subsection{Support Vector Machine}

El método Support Vector Machine en adelante llamado SVM se basa en la idea de proyectar los datos en un espacio dimensional mayor, con el fin de encontrar un hiperplano que permita separar las clases. Las características del hiperplano aseguran una alta capacidad de generalización de este método de machine learning \citep{cortes1995support}.
\\
Una vez encontrado el hiperplano, se busca un márgen $m$ que maximice el espacio entre las clases. Los vectores que se encuentran a distancia $m$ del hiperplano son llamados \textit{support vectors} (vectores de soporte).
\figura{Bitmap/03/SVM}{width=1\textwidth}{capMetodos:fig:SVM}%
{Separacion con margen. Los vectores p1 y p3 son los llamados \textit{support vectors}}

Dado que es necesario calcular productos internos entre vectores, es fácil inferir que en espacios de grandes dimensiones los mismos resultan computacionalmente costosos. Por ello, se utilizan funciones de kernel que simplifican el cálculo. Los kernels son funciones que permiten calcular el producto interno de dos vectores pertenecientes a una dimensión $n$ en una dimensión $m$, con $m>n$, trabajando en la misma dimensión $n$.
\\
Existen diferentes funciones de kernel y este es uno de los parámetros mas importantes a definir a la hora de utilizar SVM.\\
Este método se describe en \citep{cortes1995support}.
\pagebreak
\subsection{Bayes Naive}

Este clasificador asume que las variables (\textit{features}) son independientes entre si. Esta suposición generalmente no es cierta, pero simplifica radicalmente la estimación. Es por este motivo que al metodo se lo considera ``ingenuo'' (\textit{naive}). 
\\
\textit{Bayes Naive} utiliza una función de distribución a priori, el teorema de Bayes y la suposición previa para poder predecir la función de distribución a posteriori de las variables. En \citep[cap. 11.5.1]{duda1973pattern} se detalla este proceso.
\\\\
Los mejores resultados con \textit{Bayes Naive} se obtienen cuando la dimensión del espacio de \textit{features} es elevada, debido a que la estimación de la densidad se vuelve poco práctica. \citep[p. 210 - 211]{hastie2009elements}
\\\\
En \citep{kononenko1993successive} se describe al método como rápido, incremental y apto para el trabajo con atributos discretos y continuos. Sus ventajas son la excelente performance para resolver problemas de la vida real, con la posibilidad de explicar sus decisiones como la suma de la información obtenida. Sin embargo, su ingenuidad puede derivar en un bajo rendimiento cuando se tratan dominios con fuertes dependencias entre los atributos.