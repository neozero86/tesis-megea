%---------------------------------------------------------------------
%
%                          Apéndice B
%
%---------------------------------------------------------------------

\chapter[Configuración de los algoritmos]{Configuración de los algoritmos estadísticos de aprendizaje automático}
\label{apendiceConfiguracion}

Se utilizó el paquete \textit{Statistics and Machine Learning Toolbox} de la herramienta MATLAB para el uso de los algoritmos LDA, Bayes Naive y SVM.\\
Para Random Forest, no se utilizó la versión nativa de la herramienta debido a la lentitud de la misma. En su lugar se optó una implementación optimizada para MATLAB del código original en R. La misma es pública, de código abierto y se encuentra disponible para descargar en\\ https://code.google.com/p/randomforest-matlab/\\

Para los comandos de MATLAB, ``Xtrain'' y ``Xtest'' serán la matrices de \textit{features} de entrenamiento y test respectivamente. A su vez ``Ytrain'' e ``Ytest'' serán los vectores resultados esperados y obtenidos respectivamente.

%------------------------------------------------------------------
\section{LDA}
\label{capIterResult:sec:analisis con LDA}

En esté método solamente se configuró el uso de la versión pseudolineal.\\\\
Los comandos MatLab para ejecutarlo son:\\
daClassifier = ClassificationDiscriminant.fit(Xtrain,Ytrain,`discrimType',\\`pseudoLinear');\\
Ytest = daClassifier.predict(Xtest);
%------------------------------------------------------------------
\section{Bayes-Naive}
\label{capIterResult:sec:Análisis con Bayes-Naive}

Se entrenó un clasificador Bayes-Naive con una distribución de datos multinomial, dado que esta distribución suele tener buenos resultados para características discretas. \\
La probabilidad a priori de las clases se configuro como ``empírica'', es decir que la misma será igual a la frecuencia relativa de distribución de cada clase.\\\\
Los comandos MatLab para ejecutarlo son:\\
nbClassifier = NaiveBayes.fit(Xtrain,Ytrain,`Distribution',`mn');\\
Ytest = nbClassifier.predict(Xtest);

\section{Análisis con SVM}
\label{capIterResult:sec:Análisis con SVM}

Se entrenó un clasificador SVM con una función de kernel lineal.\\ 
El método utilizado para usar SVM fue SMO (\textit{Sequencial minimal optimization}) \citep{platt1998sequential}.\\
El parámetro de ajuste \textit{C} fue configurado en 1 que es el valor por defecto de la librería utilizada. (No se varió ya que la performance obtenida ronda el 100\% sin necesidad de ajustar el método).\\
Se configuraron 30000 iteraciones del método como máximo por cuestiones de velocidad.\\\\
Los comandos MatLab para ejecutarlo son:\\
opts = statset(`MaxIter',30000,`UseParallel',`Always');\\
svmClassifier = svmtrain(Xtrain,Ytrain,`options',opts);\\
Ytest = svmclassify(svmClassifier,Xtest);\\\\

Se probó con otras funciones de kernel (cuadrática, polinomial (grado 3), Gaussiana radial básica y perceptrón multicapa) pero con el set de datos utilizado, la performance de cada una de ellas era igual o peor que la obtenida con un kernel lineal. Dado que el mismo es computacionalmente menos costoso, se utilizó dicho kernel durante todo el trabajo. 

%------------------------------------------------------------------
\section{Análisis con Random Forest}
\label{capIterResult:sec:Análisis con Random Forest}

Se entrenó un clasificador \textit{Random Forest} con 300 árboles.
El "mtry" que indica el número máximo de variables en cada árbol se configuró en 12. El mismo se calculó como el piso de la raíz cuadrada de la cantidad de neuronas. Se aconseja calcular el "mtry" de esta forma por primera vez, y luego si fuera necesario ir variando el valor hasta obtener la mejor performance. \\\\
Los comandos MatLab para ejecutarlo son:\\
rfClassifier = classRF\_train(Xtrain,Ytrain, 300);\\
Ytest = classRF\_predict(Xtest,rfClassifier);
