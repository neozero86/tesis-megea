\makeglossaries
 
\newglossaryentry{MAA}
{
    name={Métodos de Aprendizaje Automático},
    symbol={Métodos de Aprendizaje Automático},
    text={métodos de aprendizaje automático},
    description={El aprendizaje automático es una rama de la inteligencia artificial cuyo objetivo es desarrollar técnicas capaces de generalizar comportamientos a partir de una información suministrada en forma de ejemplos. Los métodos de Aprendizaje automático consiguen a partir de un conjunto de ejemplos de entrenamiento, inferir un modelo de las categorías en las que se agrupan los datos. Esto permite que luego puedan asignar una o más categorías a nuevos ejemplos de manera automática, mediante analogías de patrones en dicho modelo}
}
 
\newglossaryentry{PAN}
{
    name={Potenciales de acción neuronales},    
    text={potenciales de acción neuronales},
    description={También llamado impulso eléctrico, es una onda de descarga eléctrica que viaja a lo largo de la membrana celular modificando su distribución de carga eléctrica. Los potenciales de acción neuronales premiten la liberación de un neurotransmisor en las sinapsis entre células del sistema nervioso}
}

\newglossaryentry{VTA}
{
    name={Área Tegmental Ventral (VTA)},
    symbol={Área Tegmental Ventral (VTA)},
    text={Área Tegmental Ventral},
    description={Grupo de neuronas localizadas cerca de la línea media del piso del mesencéfalo. El VTA es el punto de origen donde se encuentran los cuerpos de las células dopaminérgicas del sistema dopaminérgico mesocorticolímbico, y se encuentra ampliamente implicado en el sistema de recompensa natural del cerebro. El VTA contiene neuronas que se proyectan hacia numerosas áreas del cerebro, desde la Corteza PreFrontal hasta el tallo cerebral pasando por numerosas regiones entre estas dos}
}

\newglossaryentry{PFC}
{
    name={Corteza PreFrontal (PFC)},
    symbol={Corteza PreFrontal (PFC)},
    text={Corteza PreFrontal},
    description={Parte anterior de los lóbulos frontales del cerebro. Se ubica frente a las áreas motora y premotora}
}

\newglossaryentry{ID}
{
    name={Inervación dopaminérgica},
    text={inervación dopaminérgica},
    description={Hace referencia al conjunto de axones de neuronas dopaminergicas que hacen sinapsis con neuronas de la corteza}
}

\newglossaryentry{CE}
{
    name={C. Elegans},
    description={Caenorhabditis elegans es una especie de nematodo rabdítido familia Rhabditidae que mide aproximadamente 1 mm de longitud, y vive en ambientes templados. Ha sido un importante modelo de estudio para la biología, muy especialmente la genética del desarrollo, a partir de los años 70}
}

\newglossaryentry{AC}
{
    name={Áreas corticales},
    text={áreas corticales},
    description={Regiones anatómicas en las que se subdivide la corteza cerebral}
}

\newglossaryentry{CN}
{
    name={Código neuronal},
    text={código neuronal},
    description={Código que representa la información contenida en una red neuronal}
}

\newglossaryentry{NP}
{
    name={Neuroprostética},
    text={neuroprostética},
    description={Disciplina relacionada con la neurociencia y la ingeniería biomédica que estudia el desarrollo de prótesis neurales. Incluyen las interfaces cerebro-computadora}
}

\newglossaryentry{CP}
{
    name={Corteza Premotora},
    text={corteza premotora},
    description={Corteza encargada de pre-procesar movimientos y planes de movimiento}
}

\newglossaryentry{DL}
{
    name={Decodificadores Lineales},
    text={decodificadores lineales},
    description={\Glspl{C} lineales}
}

\newglossaryentry{V}
{
    name={Vibrisas},
    text={vibrisas},
    description={Tipo de cabellos rígidos especializados que poseen algunos animales (especialmente los mamíferos, a modo de bigotes) como elemento sensorial táctil}
}

\newglossaryentry{CB}
{
    name={Corteza de Barriles},
    text={corteza de barriles},
    description={Corteza cerebral presente en los roedores responsable de procesar todos los mensajes que llegan a través de las vibrisas}
}

\newglossaryentry{E}
{
    name={Electrodos},
    text={electrodos},
    description={Conductor eléctrico utilizado para hacer contacto con una parte no metálica de un circuito, por ejemplo un semiconductor, un electrolito, el vacío (en una válvula termoiónica), un gas (en una lámpara de neón), o como en este caso, con el espacio intersináptico cercano a una o mas neuronas}
}

\newglossaryentry{AEAA}
{
    name={Algoritmos de aprendizaje automático},
    text={algoritmos de aprendizaje automático},
    description={\glssymbol{MAA}}
}


\newglossaryentry{CAA}
{
    name={Clasificadores con aprendizaje automático},
    text={clasificadores con aprendizaje automático},
    description={\glssymbol{MAA}}
}

\newglossaryentry{F}
{
    name={\textit{Feature}},
    text={\textit{feature}},
    plural={\textit{features}},
    description={Característica de los datos que se utiliza como parte de la entrada de un método de clasificación. Se selecciona porque se presume que aporta información que ayuda en la discriminación de las muestras. Cada columna de la matiz de clasificación es considerada una \textit{feature} distinta. En esta tesis, la suma de los \glspl{S} para cada neurona analizada constituye una \textit{feature} para el posterior análisis de los \glspl{C}}
}

\newglossaryentry{H}
{
    name={Hiperplano},
    text={hiperplano},
    description={Es una extensión del concepto de plano. En 2 dimensiones el hiperplano es una recta, en 3 dimensiones es un plano, en mas dimensiones se llamará simplemente hiperplano.
    La ecuación de un hiperplano en un espacio n-dimensional es: $a_1x_1+a_2x_2+...+a_nx_n=c$
    Se puede ver claramente que si $n=2$ la ecuación resultante sería $a_1x_1+a_2x_2=c$. Llamando $x$ e $y$ a $x_1$ y $x_2$ respectivamente y despejando la ecuacion se puede obtener $(-a_1/a_2) x - c= y$. Reemplazando $m=-a_1/a_2$ y $b=-c$ se obtiene la ecuación de la recta $mx+b=y$ en 2 dimensiones}
}

\newglossaryentry{MC}
{
    name={Matriz de Covarianza},
    text={matriz de covarianza},
    plural={Matrices de Covarianza},
    description={Es una matriz que contiene la covarianza entre los elementos de un vector. Es la generalización natural a dimensiones superiores del concepto de varianza de una variable aleatoria escalar.
    Sea $X= \begin{bmatrix}
  X_1 \\
  \vdots \\
  X_n
 \end{bmatrix}$\\\\
 Si $X_1\hdots X_n$ son variables aleatorias, cada una con varianza finita, la posición $(i,j)$ de la matriz de covarianza es:\\
 $\Sigma_{i,j} = E[(X_i-u_i)(X_j-u_j)]$\\ 
 donde $u_i = E(X_i)$ es el valor esperado de la entrada i-ésima del vector X}
 }

\newglossaryentry{MMC}
{
    name={Matriz mal condicionada},
    text={mal condicionada},
    description={El número $K(A)$ se llama número condicional de la matriz $A$. 
    Se define $K(A) = \| A \| \| A^{-1} \|$
    Una matriz $A$ está bien condicionada si $K(A)$ se aproxima a $1$ y está mal condicionada si $K(A)$ es significativamente mayor que $1$}
}

\newglossaryentry{MPI}
{
    name={Pseudoinversa de una matriz},
    text={pseudoinversa},
    description={Dada una matriz A, se define como pseudoinversa $A^+$, a una matriz que multiplicada por la derecha por A da la matriz diagonal con todos los elementos diagonales iguales a 1; es decir: $I=A A^+$}
}

\newglossaryentry{S}
{
    name={\textit{Spikes}},
    text={\textit{spike}},
    plural={\textit{spikes}},
    description={\glslink{PAN}{Potenciales de acción}}
}

\newglossaryentry{ACM}
{
    name={Área cortical motora},
    text={área cortical motora},
    description={Zona donde se producen las respuestas que son reflejadas por los órganos efectores}
}

\newglossaryentry{PCA}
{
    name={Análisis de componentes principales},
    text={análisis de componentes principales},
    description={Técnica utilizada para reducir la dimensionalidad de un conjunto de datos. La técnica sirve para hallar las causas de la variabilidad de un conjunto de datos y ordenarlas por importancia}
}

\newglossaryentry{DQ}
{
    name={Técnica divide y conquista},
    text={técnica divide y conquista},
    description={Hace referencia a un refrán que implica resolver un problema difícil, dividiéndolo en partes más simples tantas veces como sea necesario, hasta que la resolución de las partes se torna obvia. La solución del problema principal se construye con las soluciones encontradas}
}

\newglossaryentry{AD}
{
    name={Árboles de decisión},
    text={árboles de decisión},
    description={Método de aprendizaje supervizado no paramétrico utilizado para clasificación y regresión. El objetivo es crear un modelo que predice el valor de una variable objetivo, aprendiendo reglas de decisión simples inferidas de las \textit{features} de los datos}
}

\newglossaryentry{EG}
{
    name={Error de generalización},
    text={error de generalización},
    description={Es una función que mide la capacidad de un método de aprendizaje automático para generalizar los datos no observados, a partir de las reglas generadas en el aprendizaje, sobre los datos observados}
}

\newglossaryentry{ISI}
{
    name={\textit{Inter Spike Interval}},
    description={Tiempo medido entre dos potenciales de acción sucesivos}
}

\newglossaryentry{MDC}
{
    name={Método de clasificación},
    text={método de clasificación},
    plural={métodos de clasificación},
    description={\gls{C}}
}

\newglossaryentry{C}
{
    name={Clasificador},
    text={clasificador},
    plural={clasificadores},
    description={El término clasificador se utiliza en referencia al algoritmo utilizado para asignar un elemento entrante no etiquetado en una categoría concreta conocida. Dicho algoritmo, permite ordenar o disponer por clases elementos entrantes, a partir de cierta información característica de los mismos}
}

\newglossaryentry{MCf}
{
    name={Matriz de confusión},
    description={Herramienta de visualización que se emplea en aprendizaje supervisado. Cada columna de la matriz representa el número de predicciones de cada clase, mientras que cada fila representa a las instancias en la clase real}
}

\newglossaryentry{BSEM}
{
    name={Error Estándar a la Media del Bootstrapping (B.S.E.M.)},
    symbol={B.S.E.M.},
    description={Es el desvío estándar de todas las muestras generadas con el método de Bootstrapping}
}

\newglossaryentry{CA}
{
    name={Comparaciones Apareadas},
    text={comparaciones apareadas},
    description={Se compararon todos los métodos entre sí. Estas comparaciones se definen como apareadas ya que los datos de entrenamiento y de evaluación son los mismos para todos los métodos en cada ventana}
}

\newglossaryentry{DP}
{
    name={Distribución paramétrica},
    text={distribución paramétrica},
    description={Forma matemática abstracta que representa de manera concisa las variaciones en un conjunto de datos}
}

\newglossaryentry{Si}
{
    name={Significancia estadística},
    text={significancia},
    description={La significancia estadística es el número, llamado p-valor tal que la probabilidad de tomar la decisión de rechazar la hipótesis nula, cuando ésta es verdadera, no es mayor que p.
    Una hipótesis nula es una hipótesis construida para anular o refutar, con el objetivo de apoyar una hipótesis alternativa}
}

\newacronym{FPGA}{FPGA}{(del inglés Field Programmable Gate Array) es un dispositivo semiconductor que contiene bloques de lógica cuya interconexión y funcionalidad puede ser configurada 'in situ' mediante un lenguaje de descripción especializado. La lógica programable puede reproducir desde funciones tan sencillas como las llevadas a cabo por una puerta lógica o un sistema combinacional hasta complejos sistemas en un chip}

\newglossaryentry{Bs}
{
    name={\textit{Bootstrapping}},
    symbol={\textit{Bootstrapping}},
    text={\textit{bootstrapping}},
    description={Método de remuestreo propuesto por Bradley Efron en 1979 \citep{efron1979bootstrap}. Se utiliza para aproximar la distribución en el muestreo de un estadístico muestral. El método consiste en tomar muestras aleatorias con reemplazo (puede tomarse el mismo elemento mas de una vez) para estimar propiedades de una población}
}

\newglossaryentry{ALi}
{
    name={Ad Libitum},
    text={\textit{ad libitum}},
    description={Expresión del latín que significa literalmente «a placer, a voluntad»}
}

\newglossaryentry{AEK}
{
    name={Aparato estereotáxico Kopf},
    text={aparato estereotáxico Kopf},
    description={Instrumento diseñado originalmente por David Kopf Instruments en 1963, su facilidad de uso y versatilidad permiten una precisa alineación de los animales pequeños para la colocación estereotáxica de electrodos, micropipetas, cánulas y otros dispositivos}
}

\newglossaryentry{DDM}
{
    name={Distribución Multinomial},
    text={distribución de datos multinomial},
    description={La distribución multinomial es una generalización de la distribución binomial}
}

\newglossaryentry{DU}
{
    name={Distribución Uniforme},
    text={distribución uniforme},
    description={En este documento hacemos referencia a la distribución uniforme discreta. La misma se define como una distribución de probabilidad que asume un número finito de valores con la misma probabilidad.\\
    Si la distribución asume los valores reales $x_1, x_2\ldots x_n \,\!$, su función de probabilidad es:     $p(x_i)=1/n$.\\
    Por ejemplo, para un dado perfecto todos los resultados tienen la misma probabilidad $1/6$. Por consiguiente, la probabilidad de que al lanzarlo caiga 4 es $1/6$}
}

\newglossaryentry{PDC}
{
    name={Performance de clasificación},
    text={performance},
    symbol={performance de clasificación},
    description={Se encuentra determinada por la precisión de clasificación (\textit{accuracy}). La misma se calcula como:\\ (aciertos totales)/(total de casos) $ = (T1+T2)/(T1+F1+T2+F2)$ siendo T1 y T2 las veces que el clasificador predijo correctamente las clases 1 y 2 respectivamente y F1 y F2 las veces que el clasificador predijo la clase en forma incorrecta.\\
Ejemplos: 
\begin{itemize}
\item Si para una muestra de la clase 1, el \gls{C} infiere que pertenece a la clase 2, es un F2 (falso 2).
\item Si para una muestra de la clase 2, el \gls{C} infiere que pertenece a la clase 1, es un F1 (falso 1).
\item Si para una muestra de la clase 1, el \gls{C} infiere que pertenece a la clase 1, es un T1 (acierto 1).
\item Si para una muestra de la clase 2, el \gls{C} infiere que pertenece a la clase 2, es un T2 (acierto 2).
\end{itemize}
	Dado que en este documento, los \glspl{C} están trabajando sobre set de datos de \gls{PAN}, se utilizan ``performance en la decodificación'' o ``performance de decodificación'' como sinónimos de performance de clasificación}
}

\newglossaryentry{PCSN}
{
    name={Precisión de clasificación},
    text={precisión},
    symbol={precisión de clasificación},
    description={La precisión o \textit{accuracy} para una clasificación con dos clases, se calcula como: $(aciertos de ambas clases)/(total de casos) = (T1+T2)/(T1+F1+T2+F2)$ siendo T1 y T2 las veces que el clasificador predijo correctamente las clases 1 y 2 respectivamente y F1 y F2 las veces que el clasificador predijo la clase en forma incorrecta.\\
Ejemplos: 
\begin{itemize}
\item Si para una muestra de la clase 1, el \gls{C} infiere que pertenece a la clase 2, es un F2.
\item Si para una muestra de la clase 2, el \gls{C} infiere que pertenece a la clase 1, es un F1.
\item Si para una muestra de la clase 1, el \gls{C} infiere que pertenece a la clase 1, es un T1.
\item Si para una muestra de la clase 2, el \gls{C} infiere que pertenece a la clase 2, es un T2.
\end{itemize}}
}